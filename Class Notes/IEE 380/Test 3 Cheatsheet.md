# CAREFULLY LOOK AT QUESTIONS!!! LOOK AT THE UNITS UNITS UNITS!!! RAAAAAAAAAA  

**CH 1**: 
- **Variability:** when successive observations of a system/phenomenon don't produce the same result.
- **Random Variable:** a measurement that exhibits variability (for random variable X, $X = \mu + \epsilon$, where $\mu$ is a constant and $\epsilon$ is a random disturbance; See Ch. 2 for more details
- **Retrospective Study:** use all/sample of historical data - involves significant amount of data, but said data may be poor or missing.
- **Observational Study:** observe current process and record quantities of interest without interrupting population - resolves precision problem of previous method.
- **Designed Experiment:** make *deliberate* or *purposeful* changes in the control variables, observe the output, and then make an inference/decision on which control variables are responsible for which outputs.
- **Overcontrol/Tampering:** adjustments to a process based on random disturbances that can actually *increase* the variation.
- **Mechanistic Model:** a model developed from theoretical knowledge or experience. 
- **Empirical Model:** a model to relate a response to one or more regressors that is developed from data obtained from the system.

**CH 2:** 
- **Model and Physical System:** Physical system provides measurements to the model, model predicts response to control inputs based on measurements.
- **Random Experiment:** can result in different outcomes, even though it is repeated in the same manner.
- **Sample Space:** The set of all possible outcomes of a random experiment, denoted as $S$. *Examples*: $\{x|x>0\}$, $\{low, medium, high\}$...
	- **Discrete:** if $S$ consists of a finite or countably infinite set of outcomes.
	- **Continuous:** if $S$ contains an interval (either finite or infinite) of real numbers.
- **Event:** a subset of the sample space of a random experiment. **Mutually Exclusive:** if $E_1 \cap E_2 = \emptyset$.   
- **Basic Set Operations:** $E_1 \cup E_2$ - *union*, all outcomes that are contained in either event | $E_1 \cap E_2$ - *intersection*, all outcomes that are contained in both events | $E'; E^C$ - *complement*, all outcomes that are not in the event | $E_1 - E_2 \equiv E_1 \cap E_2'$ - *difference*, all outcomes contained in $E_1$ but not in $E_2$.
- **Multiplication Rule (for counting techniques):** If an operation has $k$ steps, and the number of ways to complete step $i$ is $n_i$, the total number of ways to complete the operation is $n_1 \times n_2 \times \cdots \times n_k$.
- **Permutations:** number of ordered sequences of $r$ elements that can be selected from $n$ elements: $\frac{n!}{(n-r)!}$ | if $n = n_1 + n_2 + \cdots + n_r$ objects of which $n_i$ are of the $i$th type, use this: $\frac{n!}{n_1!n_2!...n_r!}$ 
- **Combinations:** number of **unordered** subsets of $r$ elements that can be selected from a set of $n$ elements: ${n\choose r}=\frac{n!}{r!(n-r)!}$ | use for *sampling without replacement*.
- **Equally Likely Outcomes:** when a sample space consists of $N$ outcomes and the probability of each is $\frac{1}{N}$.
- **Probability of an Event:** $P(E)$, the sum of the probabilities of the outcomes in $E$. $P(E) = (\text{number of outcomes in }A)/N$, where $N$ is number of outcomes in sample space.
- **Axioms of Probability:** *1.* $P(S) = 1$ where $S$ is the sample space. *2.* $0 \le P(E) \le 1$ for any event $E$. *3.* For events $E_1, E_2$ with $E_1 \cap E_2 = \emptyset$ (**MUTUALLY EXCLUSIVE**), $P(E_1 \cup E_2) = P(E_1) + P(E_2)$ 
- **Event Addition/Union Probability:** if not mutually exclusive, $P(A \cup B) = P(A) + P(B) - P(A \cap B)$; if mutually exclusive, $P(E_1 \cup E_2 \cup \ldots \cup E_k) = P(E_1) + P(E_2) + \cdots + P(E_k)$
- **Conditional Probability:** the probability of an event given that the random experiment produces an outcome in another event - $P(B|A) = \frac{P(A \cap B)}{P(A)}$ = conditional probability of $B$ given $A$. It can be seen as the relative frequency of event $B$ among the trials that produce an outcome in event $A$.
- **Random Sample:** at each step of the sample, the items that remain in the batch are equally likely to be selected.
- **Event Multiplication/Intersection Probability:** $P(A \cap B) = P(B|A)P(A) = P(A|B)P(B)$ | $P(B) = P(B|A)P(A) + P(B|A')P(A')$ | **Exhaustive sets:** $E_1 \cup E_2 \cup \ldots \cup E_k = S$ where $S$ is the sample space; if they are also mutually exclusive, $P(B) = P(B|E_1)P(E_1) + P(B|E_2)P(E_2) + \cdots + P(B|E_k)P(E_k)$
- **Independence:** *two events:* *1.* $P(A|B) = P(A)$ *2.* $P(B|A) = P(B)$ *3.* $P(A \cap B) = P(A)P(B)$; *multiple events:* for any subset of $E_1, E_2, \ldots, E_n$ events, $P(E_{i_1} \cap E_{i_2} \cap \cdots \cap E_{i_k}) = P(E_{i_1}) \times P(E_{i_2}) \times \cdots \times P(E_{i_k})$
- **Bayes' Theorem:** if $E_1, E_2, \ldots, E_k$ are mutually exclusive and exhaustive events and $B$ is any event, $P(E_1|B) = \frac{P(B|E_1)P(E_1)}{P(B|E_1)P(E_1) + P(B|E_2)P(E_2) + \cdots + P(B|E_k)P(E_k)}$
- **Random Variable:** a function that assigns a real number to each outcome in the sample space of a random experiment. **Discrete:** has a finite or countably infinite range. **Continuous:** has an interval of real numbers (either finite or infinite) for its range.

**CH 3.**
- **Probability Distribution of X:** a description of the probabilities associated with the possible values of X; for discrete, often just a list of the possible values + an associate probability.
- **Discrete Details:** 
	- *probability mass function (pmf):* ***1.*** $p(x_i)$, denoted as $P(X=x_i)$. ***2.*** the $=$ matters: $P(X\ge3) = P(X > 2)$ ***3.*** CDF(cumulative distributive function), all probabilities summed $F(x)=\sum_{Xmin}^x p(Xmin)+p(Xmin+1)\cdots p(x)$ denoted as $P(X \le x)$; $P(X=3) = P(X \le 3) - P(X \le 2)$
	- $\mu=E(X) = \sum_x xf(x)$; $\sigma^2 = V(X) = E(X-\mu)^2 = E(X^2)-(E(X))^2$; $E[h(X)] = \sum_x h(x)f(x)$; $E(aX+b) = aE(X)+b$; $V(aX+b)=a^2V(X)$
- **Discrete Uniform**: Finite \# of possible values with equal probability; $p(x_1)=p(x_2)...=p(x_n) = \frac{1}{n}$; $E(X) = \frac{b+a}{2}$; $V(X) = \frac{(b-a+1)^2-1}{12}$
- **Bernoulli trial**: Only 2 possible outcomes (success and failure); trials are independent; probability of each outcome in each trial is constant. $E(X) = p$; $V(X) = p(1-p)$; finite range
- **Binomial**: Consists of $n$ Bernoulli trials(See above for constraints); $p(x) = \binom{n}{x} p^x (1-p)^{n-x}$, $x = 0,1,\ldots ,n$; $E(X) = np$; $V(X) = np(1-p)$; finite range
- **Poisson Process**: consider subintervals of small length $\Delta t$ and assume as $\Delta t$ tends to zero:
	1. Probability of more than one event in a subinterval tends to zero.
	2. The probability of one event in a subinterval tends to $\lambda \Delta t$.
	3. The event in each subinterval is independent of other subintervals.
- **Poisson Distribution:** the random variable X that equals the number of events in a Poisson process is a Poisson random variable with parameter $\lambda > 0$ (number of events per unit length), and $p(x) = \frac{e^{-\lambda T}(\lambda T)^x}{x!}$, $x = 0, 1, 2, \ldots$ ; countably infinite range; $E(X) = \lambda T$; $V(X) = \lambda T$

**CH 4.** 
- **Continuous Details:** 
	- *probability density function (pdf):* ***1.*** $f(x)$, denoted by $P(X = x)$, which is always equal to 0. ***2.*** the $=$ doesn't matter: $P(X\ge3) = P(X>3)$ ***3.*** CDF = integration from -inf to x. ***4.*** any particular measurement can be interpret as the rounded value of a measurement that is actually in a small range, so you can actually integrate.
	- $P(a \le X \le b) = \int_a^b f(x)dx = F(b) - F(a)$; $F(x) = P(X \le x) = \int_{-\infty}^x f(u)du$; $\mu = E(X) = \int^\infty_{-\infty} xf(x)dx$; $\sigma^2=V(X) = \int^\infty_{-\infty} x^2f(x)dx-\mu^2$; $f(x) = \frac{dF(x)}{dx}$ 
- **Continuous Uniform**: $f(x) = \frac{1}{b-a}, \ a \le x \le b$; $E(X) = \frac{a+b}{2}$; $V(X) = \frac{(b-a)^2}{12}$; $F(X) = \frac{x-a}{b-a} \ \text{if} \ a \le x < b$ | $0 \ \text{if} \  x < a$ | $1 \ \text{if} \  b \le x$
- **Normal**: random variable with pdf $f(x) = \frac{1}{\sqrt{2\pi}\sigma} e^\frac{-(x-\mu)^2}{2\sigma^2}$, $-\infty < x< \infty$; $E(X) = \mu$, $V(X) = \sigma^2$. $N(\mu, \sigma^2)$ is used to denote the distribution.
- **Standard Normal:** Normal random variable w/ $\mu = 0$ and $\sigma^2=1$, denoted as $Z$; $\frac{X-\mu}{\sigma} = Z$; $\frac{x-\mu}{\sigma} = z$(zscore); $\Phi(z)= P(Z \le z)$. Suppose that X is a normal random variable with mean $\mu$ and variance $\sigma^2$. Then, $P(X \le x) = P(Z \le z)$, where $Z$ and $z$ are the *standardized* versions of $X$ and $x$. 
- **Exponential:** random variable $X$ equals **distance between successive events** from a Poisson process with mean number of events $\lambda$ per unit interval.
	1. $f(x) = \lambda e^{-\lambda x}$ for $0 \le x < \infty$; $E(X) = \frac{1}{\lambda}; V(X) = \frac{1}{\lambda^2}$; **REMEMBER TO USE CONSISTENT UNITS**; $F(x) = P(X \le x) = 1 - e^{-\lambda x}$
	2. Lack of Memory Property: $P(X < t_1 + t_2 | X>t_1) = P(X < t_2)$

**CH 6.**
- **Sample Mean:** if the $n$ observations in a sample are denoted by $x_1, x_2, \ldots, x_n$, the sample mean is $\bar{x} = \frac{\sum^n_{i=1}x_i}{n}$. Mean $\mu$ = average of all the measurements in the population, sample mean $\bar{x}$ = reasonable estimate of $\mu$.
- **Sample Variance/STDEV:** if $x_1, x_2, \ldots, x_n$ is a sample of $n$ observations, the sample variance is $s^2 = \frac{\sum^n_{i=1}(x_i - \bar{x})^2}{n-1}=\frac{\sum^n_{i=1} x_i^2 -\frac{(\sum^n_{i=1}x_i)^2}{n}}{n-1}$ and the stdev is $s$. The sample variance $s^2$ is an estimate of the population variance $\sigma^2$.
- **Sample Range:** if the $n$ observations in a sample are denoted by $x_1, x_2, \ldots, x_n$, $r = max(x_i) - min(x_i)$. 
- **Frequency Distribution:** divide the range of the data into intervals (cells/bins); bins should be equal width if possible; bin \# = $\sqrt{n}$. We want the lower bin limit to be slightly below the smallest and the upper bin limit to be slightly above the largest.
- **Histogram:** visual representation of frequency distribution - ***1.*** Label the bin boundaries on a horizontal scale ***2.*** Mark and label the vertical scale with the frequencies or the relative frequencies ***3.*** Above each bin, draw a rectangle where height is equal to the frequency/relative frequency corresponding to that bin. **Unequal bin widths:** if extreme observations/outliers exist -> rectangle **area** should be proportional to bin frequency, $\text{Rectangle height} = \frac{\text{Bin frequency}}{\text{Bin width}}$. **Shape:** *symmetric:* the mean and median coincide. *unimodal:* symmetric and mode coincides with the rest, one 'hump'. *skewed/asymmetrical:* right/positive skew = sloping down to the right, left/negative skew = sloping down to the left.

**CH 7.**
- **Statistical Inference:** using statistical methods to make decisions and draw conclusions about environments - *Parameter Estimation* and *Hypothesis Testing*.
- **Statistic:** Suppose that we want to obtain a point estimate of a population parameter. We know that the observations are considered to be random variables; therefore, any function of the observations, or *statistic*, is also a random variable, e.g. the sample mean/variance. *TLDR:* any function of the observations in a random sample.
- **Sampling Distribution:** The probability distribution of a statistic.
- **Point Estimate:** a single numerical value $\hat{\theta}$ which estimates a population parameter $\theta$, and which is a single numerical value of a statistic $\hat{\Theta} = h(X_1, X_2, \ldots, X_n)$ (*a point estimator*).
- **Reasonable Point Estimates:** The mean $\mu$ of a single population: $\bar{x}$ | The variance $\sigma^2$ (or stdev $\sigma$) of a single population: $s^2, s$ | The proportion p of items in a population that belong to a class of interest: $\hat{p} = \frac{x}{n}$, where $x$ is the \# of items in a random sample of size $n$ that belongs to the class of interest | the difference in means of two populations $\mu_1 - \mu_2$: $\bar{x}_1-\bar{x}_2$ | The difference in two population proportions, $p_1 - p_2$: $\hat{p}_1-\hat{p}_2$  
- **Random Sample:** the random variables $X_1, X_2, \ldots, X_n$ are a *random sample* of size $n$ if (a) the $X_i$'s are independent random variables and (b) every $X_i$ has the same probability distribution.
- **Central Limit Theorem:** if $X_1, X_2, \ldots, X_n$ is a random sample of size $n$ taken from a population (either finite or infinite) with mean $\mu$ and finite variance $\sigma^2$ and if $\overline{X}$ is the sample mean, the limiting form of the distribution of $Z = \frac{\overline{X}- \mu}{\sigma/\sqrt{n}}$ as $n \rightarrow \infty$, is the **standard normal distribution**. $\mu_{\overline{X}}= \mu$, $\sigma^2_{\overline{X}} = \frac{\sigma^2}{n}$; $n>30$ recommended by the book, $n>40$ recommended by the professor.
- **Approximate Sampling Distribution of a Difference in Sample Means:** $Z= \frac{\overline{X}_1-\overline{X}_2 - (\mu_1-\mu_2)}{\sqrt{\sigma_1^2/n_1 + \sigma_2^2/n_2}}$ is approximately standard normal if the conditions of the *central limit theorem* apply. If the two populations are normal, the sampling distribution of $Z$ is exactly standard normal.

**CH 8.**
- **Confidence Interval:** a range of plausible values for the point estimator in question; always specifies a confidence level of $100(1-\alpha)\%$, which is a measure of the reliability (*if repeated, $100(1-\alpha)\%$ of the samples would produce a confidence interval that contains the true population parameter*) of the procedure. The *length* of a CI is a measure of the *precision* of the estimation.
	- As the desired length of the interval decreases, the require sample size $n$ increases for a fixed value of $\sigma$ and specified confidence.
	- As $\sigma$ increases, the require sample size $n$ increases for a fixed desired length and specified confidence.
	- As the level of confidence increases, the required sample size $n$ increases for fixed desired length and stdev $\sigma$. 
- $t$ **Distribution:** $X_1, X_2, \ldots, X_n$ is a random sample from a normal distribution with unknown mean $\mu$ and unknown variance $\sigma^2$. The random variable $T - \frac{\overline{X}-\mu}{S/\sqrt{n}}$ has a $t$ distribution with $n-1$ dof.
- $\chi^2$ **Distribution:** $X_1, X_2, \ldots, X_n$ is a random sample from a normal distribution with unknown mean $\mu$ and unknown variance $\sigma^2$, and let $S^2$ be the sample variance. Then the random variable $X^2 - \frac{(n-1)S^2}{\sigma^2}$ has a chi-square distribution with $n-1$ dof. Variance = 2\*dof.
- **Normal Approximation for a Binomial Proportion:** If $n$ is large ($np>5, n(1-p)>5$), the distribution of $Z = \frac{X-np}{\sqrt{np(1-p)}}=\frac{\hat{P}-p}{\sqrt{\frac{p(1-p)}{n}}}$ is approximately standard normal.
- **Roadmap for Constructing CIs, One-Sample:**

| Parameter to be Bounded by CO or Tested w/ Hypothesis?                                                                                                                                         | Symbol     | Confidence Interval                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Choice of Sample Size                                                                        |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------- |
| **Mean of normal distribution:**  $\bar{x}$ is the sample mean of a random sample of size $n$ from a normal population with known variance $\sigma^2$                                          | $\mu$      | A $100(1-\alpha)\%$ confidence interval on $\mu$ is given by $\bar{x} - z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \le \mu \le \bar{x} + z_{\alpha/2} \frac{\sigma}{\sqrt{n}}$, where $z_{\alpha/2}$ is the upper $100\alpha/2$ percentage points of the standard normal distribution. *One-Sided:* *upper*: $\mu \le \bar{x} + z_{\alpha} \frac{\sigma}{\sqrt{n}}$; *lower*: $\bar{x} - z_{\alpha} \frac{\sigma}{\sqrt{n}} \le \mu$                                                                                                                   | $n = (\frac{z_{\alpha/2}\sigma}{E})^2$                                                       |
| **Mean of arbitrary distribution with large sample size:** Central limit theorem applies and $\sigma$ is essentially known                                                                     | $\mu$      | When $n$ is large, the quantity $\frac{\overline{X}-\mu}{S/\sqrt{n}}$ has an approximate standard normal distribution. Consequently, $\overline{X} - z_{\alpha/2} \frac{s}{\sqrt{n}} \le \mu \le \overline{X} + z_{\alpha/2} \frac{s}{\sqrt{n}}$ where $s$ is the sample stdev is CI with confidence level of approximately $100(1-\alpha)\%$. Same *upper* and *lower* as above.                                                                                                                                                                | $n \ge 40$                                                                                   |
| **Mean of normal distribution:** $\bar{x}$ and $s$ are the mean and standard deviation of a random sample from a normal distribution with unknown variance $\sigma^2$                          | $\mu$      | A $100(1-\alpha)\%$ confidence interval on $\mu$ is given by $\bar{x} - t_{\alpha/2,n-1} \frac{s}{\sqrt{n}} \le \mu \le \bar{x} + t_{\alpha/2,n-1} \frac{s}{\sqrt{n}}$, where $t_{\alpha/2,n-1}$ is the upper $100\alpha/2$ percentage points of the $t$ distribution with $n-1$ dof. *One-Sided:* same *upper* and *lower* as above.                                                                                                                                                                                                            | Can only be obtained through trial and error.                                                |
| **Variance (or standard deviation) of normal distribution:** $s^2$ is the sample variance from a random sample of $n$ observations from a normal distribution with unknown variance $\sigma^2$ | $\sigma^2$ | A $100(1-\alpha)\%$ confidence interval on $\sigma^2$ is given by $\frac{(n-1)s^2}{\chi^2_{\alpha/2,n-1}} \le \sigma^2 \le \frac{(n-1)s^2}{\chi^2_{1-\alpha/2,n-1}}$, where $\chi^2_{\alpha/2,n-1}$ and $\chi^2_{1-\alpha/2,n-1}$ are the upper and lower $100\alpha/2$ percentage points of the chi-square distribution with $n-1$ dof, respectively. Square root for stdev limits.*One-Sided:* same *upper* and *lower* as above, replace $\alpha/2$ with $\alpha$ and get rid of the other side. **Less robust to the normality assumption.** | Same as above.                                                                               |
| **Population proportion:** $\hat{p}$ is the proportion of observations in a random sample of size $n$ that belongs to a class of interest                                                      | $p$        | An approximate $100(1-\alpha)\%$ confidence interval on the proportion $p$ is of the population that belongs to this class is $\hat{p} - z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \le p \le \hat{p} + z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$, where $z_{\alpha/2}$ is the upper $100\alpha/2$ percentage points of the standard normal distribution. Same *upper* and *lower* as above.                                                                                                                                         | $n = (\frac{z_{\alpha/2}}{E})^2p(1-p)$; **No estimation:** $(\frac{z_{\alpha/2}}{E})^2 0.25$ |

**CH 9.**
- **Statistical Hypothesis:** a statement about the parameters of one or more populations. *null hypothesis:* $H_0$, initially assumed to be true. *alternative hypothesis:* $H_1$, contradicts the null hypothesis.
- **Acceptance Region:** a region in the sample space of the test statistic such that if the test statistic falls within it, the null hypothesis cannot be rejected. **This terminology is used because rejection of $H_0$ is always a strong conclusion and acceptance of $H_0$ is generally a weak conclusion.** 
- **Critical value(s):** the value of a statistic corresponding to a stated *significance level* as determined from the sampling distribution. For example, if $P(Z \ge z_{0.05}) = P(Z\ge 1.96)$, then $z_{0.05} = 1.96$ is the critical value of $z$.
- **Type I Error:** Rejecting the null hypothesis when it is true. *Probability:*  $\alpha = P(\text{type I error})$. This value is often called the *significance level*, $\alpha$-*error*, or the *size of the test*. The area of the region outside the critical values.
- **Type II Error:** Failing to reject the null hypothesis when it is false. *Probability:* $\beta = P(\text{type II error})$. This value is often called the $\beta$-*error*. The area of the true parameter's probability distribution that falls in the *acceptance region* of the null hypothesis' parameter's probability distribution.
1. The size of the critical region, and consequently the probability of a type I error α, can always be reduced by appropriate selection of the critical values.
2. Type I and type II errors are related. A decrease in the probability of one type of error always results in an increase in the probability of the other provided that the sample size n does not change.
3. An increase in sample size reduces β provided that α is held constant.
4. When the null hypothesis is false, β increases as the true value of the parameter approaches the value hypothesized in the null hypothesis. The value of β decreases as the difference between the true mean and the hypothesized value increases.
- **Power:** the probability of rejecting the null hypothesis $H_0$ when the alternative hypothesis is true for a specific statistical test. $1-\beta$, *the probability of correctly rejecting a false null hypothesis*. Measures the *sensitivity* of a statistical test - the ability to detect differences.
- In formulating one-sided alternative hypotheses, we should remember that rejecting H0 is always a strong conclusion. Consequently, we should put the statement about which it is important to make a strong conclusion in the alternative hypothesis. In real-world problems, this will often depend on our point of view and experience with the situation.
- **P-Value:** the smallest level of significance that would lead to rejection of the null hypothesis $H_0$ with the given data. The probability that a similar conclusion would be reached given the null hypothesis is true.
- **General Procedure for Hypothesis Tests:**
	1. **Parameter of interest:** Identify the parameter of interest from the problem context.
	2. $H_0$: State the null hypothesis.
	3. $H_1$: Specify an appropriate alternative hypothesis.
	4. **Test statistic:** Determine an appropriate test statistic.
	5. **Reject $H_0$ if**: State the rejection criteria for the null hypothesis.
	6. **Computations:** Compute any necessary sample quantities, substitute these into the equation for the test statistic, and compute that value.
	7. **Draw conclusions:** Decide whether or not $H_0$ should be rejected and report that in the problem context.

| Case | Null Hypothesis                       | Test Statistic                                | Alternative Hypothesis          | Fixed Significance Level Criteria for Rejection                              | P-Value                                                                                                                      |
| ---- | ------------------------------------- | --------------------------------------------- | ------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| 1.   | $H_0: \mu = \mu_0$ $\sigma^2$ known   | $z_0 = \frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}}$ | $H_1: \mu \neq \mu_0$           | $\|z_0\| > z_{\alpha/2}$                                                     | $P=2[1-\Phi(z_0)]$                                                                                                           |
|      |                                       |                                               | $H_1: \mu > \mu_0$              | $z_0 > z_{\alpha}$                                                           | $1-\Phi(z_0)$                                                                                                                |
|      |                                       |                                               | $H_1: \mu < \mu_0$              | $z_0 < -z_{\alpha}$                                                          | $\Phi(z_0)$                                                                                                                  |
| 2.   | $H_0: \mu = \mu_0$ $\sigma^2$ unknown | $t_0 = \frac{\bar{x}-\mu_0}{s/\sqrt{n}}$      | $H_1: \mu \neq \mu_0$           | $\|t_0\| > t_{\alpha/2, n-1}$                                                | Probability above $\|t_0\|$ **and** below $-\|t_0\|$                                                                         |
|      |                                       |                                               | $H_1: \mu > \mu_0$              | $t_0 > t_{\alpha, n-1}$                                                      | Probability above $t_0$                                                                                                      |
|      |                                       |                                               | $H_1: \mu < \mu_0$              | $t_0 < t_{\alpha, n-1}$                                                      | Probability below $t_0$                                                                                                      |
| 3.   | $H_0: p = p_0$                        | $z_0 = \frac{x-np_0}{\sqrt{np_0(1-p_0)}}$     | $H_1: p \neq p_0$               | $\|z_0\| > z_{\alpha/2}$                                                     | $P=2[1-\Phi(z_0)]$                                                                                                           |
|      |                                       |                                               | $H_1: p > p_0$                  | $z_0 > z_{\alpha}$                                                           | $1-\Phi(z_0)$                                                                                                                |
|      |                                       |                                               | $H_1: p < p_0$                  | $z_0 < -z_{\alpha}$                                                          | $\Phi(z_0)$                                                                                                                  |
| 4.   | $H_0: \sigma^2 = \sigma^2_0$          | $x_0^2=\frac{(n-1)s^2}{\sigma^2_0}$           | $H_1: \sigma^2 \neq \sigma^2_0$ | $\chi^2_0 > \chi^2_{\alpha/2, n-1}$ or $\chi^2_0 < \chi^2_{1-\alpha/2, n-1}$ | if $\chi^2_{\alpha_1,dof} = L$ and $\chi^2_{\alpha_2,dof}=R$, and $L < \chi^2_0 < R$, $\alpha_2 < \text{P-value} < \alpha_1$ |
|      |                                       |                                               | $H_1: \sigma^2 > \sigma^2_0$    | $\chi^2_0 > \chi^2_{\alpha, n-1}$                                            | Area in the upper tail of the chi-square distribution to the right of $\chi^2_0$, **double for two-sided.**                  |
|      |                                       |                                               | $H_1: \sigma^2 < \sigma^2_0$    | $\chi^2_0 < \chi^2_{1-\alpha, n-1}$                                          | Area in the lower tail of the chi-square distribution to the left of $\chi^2_0$                                              |

**Two-Sided Test on the Mean, Variance Known:**
$\beta = \Phi(z_{\alpha/2} - \frac{\delta \sqrt{n}}{\sigma}) - \Phi(-z_{\alpha/2} - \frac{\delta \sqrt{n}}{\sigma})$, $\delta = \mu - \mu_0$, 
two sided: $n = \frac{(z_{\alpha/2}+z_\beta)^2\sigma^2}{\delta^2}$ one sided: $n = \frac{(z_{\alpha}+z_\beta)^2\sigma^2}{\delta^2}$ 
**Two-Sided Test on a Binomial Proportion:**
$\beta = \Phi(\frac{p_0-p+z_{\alpha/2}\sqrt{p_0(1-p_0)/n}}{\sqrt{p(1-p)/n}})- \Phi(\frac{p_0-p-z_{\alpha/2}\sqrt{p_0(1-p_0)/n}}{\sqrt{p(1-p)/n}})$ 
$H_0:p<p_0$: $\beta = 1 - \Phi(\frac{p_0-p-z_{\alpha}\sqrt{p_0(1-p_0)/n}}{\sqrt{p(1-p)/n}})$
$H_0:p>p_0$: $\beta = \Phi(\frac{p_0-p+z_{\alpha}\sqrt{p_0(1-p_0)/n}}{\sqrt{p(1-p)/n}})$
two-sided: $n = [\frac{z_{\alpha/2}\sqrt{p_0(1-p_0)}+z_\beta \sqrt{p(1-p)}}{p-p_0}]^2$; one-sided: $n = [\frac{z_{\alpha}\sqrt{p_0(1-p_0)}+z_\beta \sqrt{p(1-p)}}{p-p_0}]^2$

**CH 10.**

**Summary of Two-Sample Hypothesis-Testing Procedures:**

| Case | Null Hypothesis                                                     | Test Statistic                                                                               | Alternative Hypothesis            | Fixed Significance Level Criteria for Rejection                          | P-Value                                                                                                       |
| ---- | ------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- | --------------------------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------- |
| 1.   | $H_0: \mu_1-\mu_2 = \Delta_0$; $\sigma^2_1$ and $\sigma^2_2$ known  | $z_0 = \frac{\bar{x}_1-\bar{x}_2-\Delta_0}{\sqrt{\sigma^2_1/n_1+\sigma^2_2/n_2}}$            | $H_1: \mu_1-\mu_2 \neq \Delta_0$  | $\|z_0\| > z_{\alpha/2}$                                                 | $P=2[1-\Phi(z_0)]$                                                                                            |
|      |                                                                     |                                                                                              | $H_1: \mu_1-\mu_2 > \Delta_0$     | $z_0 > z_{\alpha}$                                                       | $1-\Phi(z_0)$                                                                                                 |
|      |                                                                     |                                                                                              | $H_1: \mu_1-\mu_2 < \Delta_0$     | $z_0 < -z_{\alpha}$                                                      | $\Phi(z_0)$                                                                                                   |
| 2.   | $H_0: \mu_1-\mu_2 = \Delta_0$; $\sigma^2_1=\sigma^2_2$ unknown      | $t_0 = \frac{\bar{x}_1-\bar{x}_2-\Delta_0}{S_p\sqrt{1/n_1+1/n_2}}$                           | $H_1: \mu_1-\mu_2 \neq \Delta_0$  | $\|t_0\| > t_{\alpha/2, n_1+n_2-2}$                                      | Probability above $\|t_0\|$ **and** below $-\|t_0\|$                                                          |
|      |                                                                     | $S_p^2=\frac{(n_1-1)s_1^2 + (n_2-1)s^2_2}{n_1+n_2-2}$                                        | $H_1: \mu_1-\mu_2 > \Delta_0$     | $t_0 > t_{\alpha, n_1+n_2-2}$                                            | Probability above $t_0$                                                                                       |
|      |                                                                     |                                                                                              | $H_1: \mu_1-\mu_2 < \Delta_0$     | $t_0 < -t_{\alpha, n_1+n_2-2}$                                           | Probability below $t_0$                                                                                       |
| 3.   | $H_0: \mu_1-\mu_2 = \Delta_0$; $\sigma^2_1 \neq \sigma^2_2$ unknown | $t_0 = \frac{\bar{x}_1-\bar{x}_2-\Delta_0}{\sqrt{s_1^2/n_1+s_2^2/n_2}}$                      | $H_1: \mu_1-\mu_2 \neq \Delta_0$  | $\|t_0\| > t_{\alpha/2, v}$                                              | Probability above $\|t_0\|$ **and** below $-\|t_0\|$                                                          |
|      |                                                                     | $v= \frac{(s_1^2/n_1+s_2^2/n_2)^2}{\frac{(s_1^2/n_1)^2}{n_1-1}+\frac{(s_2^2/n_2)^2}{n_2-1}}$ | $H_1: \mu_1-\mu_2 > \Delta_0$     | $t_0 > t_{\alpha, v}$                                                    | Probability above $t_0$                                                                                       |
|      |                                                                     |                                                                                              | $H_1: \mu_1-\mu_2 < \Delta_0$     | $t_0 < -t_{\alpha, v}$                                                   | Probability below $t_0$                                                                                       |
| 2.   | $H_0: \mu_D = 0$; $\mu_d = \mu_1-\mu_2$                             | $t_0 = \frac{\overline{d}}{s_d/\sqrt{n}}$                                                    | $H_1: \mu_d \neq 0$               | $\|t_0\| > t_{\alpha/2, n-1}$                                            | Probability above $\|t_0\|$ **and** below $-\|t_0\|$                                                          |
|      |                                                                     | $\overline{d}$ = sample average of n differences, $s_d$ is stdev of these differences        | $H_1: \mu_d >0$                   | $t_0 > t_{\alpha, n-1}$                                                  | Probability above $t_0$                                                                                       |
|      |                                                                     |                                                                                              | $H_1: \mu_d <0$                   | $t_0 < -t_{\alpha, n-1}$                                                 | Probability below $t_0$                                                                                       |
| 5.   | $H_0: p_1 = p_2$                                                    | $z_0 = \frac{\hat{p}_1-\hat{p}_2}{\sqrt{\hat{p}(1-\hat{p})[\frac{1}{n_1}+\frac{1}{n_2}]}}$   | $H_1: p_1 \neq p_2$               | $\|z_0\| > z_{\alpha/2}$                                                 | $P=2[1-\Phi(z_0)]$                                                                                            |
|      |                                                                     | $\hat{p} = \frac{x_1+x_2}{n_1+n_2}$                                                          | $H_1: p_1 > p_2$                  | $z_0 > z_{\alpha}$                                                       | $1-\Phi(z_0)$                                                                                                 |
|      |                                                                     |                                                                                              | $H_1: p_1 < p_2$                  | $z_0 < -z_{\alpha}$                                                      | $\Phi(z_0)$                                                                                                   |
| 6.   | $H_0: \sigma^2_1 = \sigma^2_2$                                      | $f_0^2=\frac{s_1^2}{s_2^2}$                                                                  | $H_1: \sigma^2_1 \neq \sigma^2_2$ | $f_0 > f_{\alpha/2, n_1-1,n_2-1}$ or $f_0 < f_{1-\alpha/2, n_1-1,n_2-1}$ | if $f_{\alpha_1,u,v} = L$ and $f_{\alpha_2,u,v}=R$, and $L < f_0 < R$, $\alpha_2 < \text{P-value} < \alpha_1$ |
|      |                                                                     |                                                                                              | $H_1: \sigma^2_1 > \sigma^2_2$    | $f_0 > f_{\alpha, n_1-1,n_2-1}$                                          | Area in the upper tail of the chi-square distribution to the right of $f_0$, **double for two-sided.**        |
|      |                                                                     |                                                                                              | $H_1: \sigma^2_1 < \sigma^2_2$    | $f_0 < f_{1-\alpha, n_1-1,n_2-1}$                                        | Area in the lower tail of the chi-square distribution to the left of $f_0$                                    |

**Summary of Two-Sample CI Procedures:**

| Case | Problem Type                                                                                                  | Point Estimate        | Confidence Interval                                                                                                                                                                                                                            |
| ---- | ------------------------------------------------------------------------------------------------------------- | --------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1.   | Difference in two means $\mu_1$ and $\mu_2$, variances $\sigma_1^2$ and $\sigma_2^2$ known                    | $\bar{x}_1-\bar{x}_2$ | $\bar{x}_1-\bar{x}_2 - z_{\alpha/2} \sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}} \le \mu_1-\mu_2 \le \bar{x}_1-\bar{x}_2 + z_{\alpha/2} \sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}$                                          |
| 2.   | Difference in means of two normal distributions $\mu_1-\mu_2$, variances $\sigma_1^2=\sigma_2^2$ unknown      | $\bar{x}_1-\bar{x}_2$ | $\bar{x}_1-\bar{x}_2 - t_{\alpha/2,n_1+n_2-2}S_p \sqrt{\frac{1}{n_1}+\frac{1}{n_2}} \le \mu_1-\mu_2 \le \bar{x}_1-\bar{x}_2 + t_{\alpha/2,n_1+n_2-2}S_p \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}$                                                    |
| 3.   | Difference in means of two normal distributions $\mu_1-\mu_2$, variances $\sigma_1^2 \neq \sigma_2^2$ unknown | $\bar{x}_1-\bar{x}_2$ | $\bar{x}_1-\bar{x}_2 - t_{\alpha/2,v} \sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}} \le \mu_1-\mu_2 \le \bar{x}_1-\bar{x}_2 + t_{\alpha/2,v} \sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}$                                                          |
| 4.   | Differences in means of two normal distributions for paired sample $\mu_0=\mu_1-\mu_2$                        | $\overline{d}$        | $\overline{d}-t_{\alpha/2,n-1}\frac{s_d}{\sqrt{n}} \le \mu_D \le \overline{d}+t_{\alpha/2,n-1}\frac{s_d}{\sqrt{n}}$                                                                                                                            |
| 5.   | Ratio of the variances $\sigma_1^2/\sigma_2^2$ of two normal distributions                                    | $\frac{s_1^2}{s_2^2}$ | $\frac{s_1^2}{s_2^2} f_{1-\alpha/2, n_2-1, n_1-1} \le \frac{\sigma_1^2}{\sigma_2^2} \le \frac{s_1^2}{s_2^2} f_{\alpha/2, n_2-1, n_1-1}$ where $f_{1-\alpha/2, n_2-1, n_1-1} = 1/f_{\alpha/2, n_1-1, n_2-1}$                                    |
| 6.   | Difference in two proportions of two binomial parameters $p_1 - p_2$                                          | $\hat{p}_1-\hat{p}_2$ | $\hat{p}_1-\hat{p}_2-z_{\alpha/2}\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}} \le p_1 - p_2 \le \hat{p}_1-\hat{p}_2+z_{\alpha/2}\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}$  |

**CH 11.**
- **Simple Linear Regression Model:** $Y=\beta_0 + \beta_1 x + \epsilon$, regressor = $x$, regression coefficients = $\beta_0, \beta_1$.
- **Least Squares Estimates of SLRM:** $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$, $\hat{\beta}_1 = \frac{\sum^2_{i=1} y_i x_i - \frac{(\sum^2_{i=1} y_i)(\sum^2_{i=1} x_i)}{n}}{\sum^2_{i=1} x_i^2-\frac{(\sum^2_{i=1} x_i)^2}{n}}$, $n$ observations, $S_{xx}$ = denominator of previous, $S_{xy}$ = numerator of previous.
- **Residual:** $e_i = y_i - \hat{y}_i$
- **Error Sum of Squares:** $SS_E=\sum^n_{i=1}(y_i - \hat{y}_i)^2$ 
- **Estimator of Variance:** $\hat{\sigma}^2 = \frac{SS_E}{n-2}$
- $SS_E = SS_T - \hat{\beta}_1 S_{xy}$ where $SS_T = \sum_{i=1}^n (y_i - \bar{y})^2$ (**Total Sum of Squares**)
- **Regression Sum of Squares:** $SS_R = \sum^n_{i=1}(\hat{y}_i-\bar{y})^2$
- **Estimated SE of slope and intercept:** $se(\hat{\beta}_1) = \sqrt{\frac{\hat{\sigma}^2}{S_{xx}}}$ and $se(\hat{\beta}_0) = \sqrt{\hat{\sigma}^2[\frac{1}{n}+\frac{\bar{x}^2}{S_{xx}}]}$ 
- **Hypothesis Tests in Simple Linear Regression:** 
	- **Slope**: $H_0: \beta_1 = \beta_{1,0} \ \ H_1: \beta_1 \neq \beta_{1,0}$ | **Test Statistic:** $T_0 = \frac{\hat{\beta}_1-\beta_{1,0}}{se(\hat{\beta}_1)}$. Reject if $|t_0| > t_{\alpha/2,n-2}$. Same thing for **Intercept**, just replace 1 with 0.
- **Significance of Regression:** Failure to reject $H_0: \beta_1 = 0$ is equivalent to concluding that there is no linear relationship between $x$ and $Y$, or no significant effect at all on $Y$ caused by a change in $x$. 
- **Test for Significance of Regression:** $MS_R = SS_R/dof; MS_E = SS_E/dof$ (1 and n-2 usually); $F_0 = \frac{MS_R}{MS_E}$, reject $H_0$ if $f_0 > f_{\alpha,1,n-2}$ 
- **Coefficient of Determination:** $R^2 = \frac{SS_R}{SS_T} = 1-\frac{SS_E}{SS_T}$, the amount of variability in the data explained/accounted for by the regression model.
- **Sample Correlation Coefficient:** $\hat{\rho} = \frac{S_{xy}}{(S{xx}SS_T)^{1/2}}=\sqrt{R^2}$ 
- **Test for Zero Correlation:** $H_0: \rho = 0 \ \ H_1: \rho \neq 0$, Statistic: $T_0 = \frac{\hat{\rho}\sqrt{n-2}}{\sqrt{1-\hat{\rho}^2}}$, n-2 dof, reject if $|t_0| > t_{\alpha/2,n-2}$.  This test is equivalent to the one for slope.

**CH 12.**
- **Multiple Regression Model:** regression model that contains more than one regressor variable: $Y = \beta_0 + \beta_1 x_1 + \ldots \beta_k x_k + \epsilon$. As long as the terms are linearly independent, even if they are not linear themselves, this is a linear regression model. 
$\mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} \ \ \mathbf{X} = \begin{bmatrix} 1 & x_{11} & x_{12} & \cdots & x_{1k} \\ 1 & x_{21} & x_{22} & \cdots & x_{2k} \\ \vdots & \vdots & \vdots & & \vdots \\ 1 & x_{n1} & x_{n2} & \cdots & x_{nk} \end{bmatrix} \ \ \mathbf{\beta} = \begin{bmatrix} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_k \end{bmatrix} \ \ \mathbf{\epsilon} = \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}$   
- **Least Squares Estimate of** $\beta$: $\hat{\mathbf{\beta}} = (\mathbf{X'X})^{-1}\mathbf{X'y}$ 
- **Fitted Model:** $\mathbf{\hat{y}}=\mathbf{X\hat{\beta}}$, $SS_E = \mathbf{e'e}$ 
- **Estimator of Variance:** $\hat{\sigma}^2 = \frac{SS_E}{n-p}$ where $p$ is the number of parameters (the number of non-intercept $\beta$). **LOOK AT MS FOR RESIDUAL ERROR FOR THIS VALUE**
- **HYPOTHESIS TEST REQUIRES THAT THE RESIDUALS ARE NORMALLY AND INDEPENDENTLY DISTRIBUTED WITH MEAN ZERO AND VARIANCE** $\sigma^2$ 
- **Test for Significance of Regression**: determine whether a linear relationship exists between the response variable $y$ and a subset of the regressor variables $x_1, x_2, \ldots, x_k$. The appropriate hypotheses: $H_0: \beta_1 = \beta_2 = \cdots = \beta_k = 0$, $H_1: \beta_j \neq 0 \ \text{for at least one } j$. $F_0 = \frac{SS_R/k}{SS_E/(n-p)} = \frac{MS_R}{MS_E}$. Reject if $f_0 > f_{\alpha, k, n-p}$. 
- **Adjusted** $R^2$: $R^2_{adj} = 1 - \frac{SS_E/(n-p)}{SS_T/(n-1)}$
- **Tests on Individual Regression Coefficients and Subsets of Coefficients:** *individual:* $H_0: \beta_j = \beta_{j0}$, $H_1 : \beta_j \neq \beta_{j0}$,  $T_0 = \frac{\hat{\beta}_j-\beta_{j0}}{se(\hat{\beta}_j)}$, $se(\hat{\beta}_j) = \sqrt{\sigma^2 C_{jj}}$ where $C_{jj}$ is the diagonal element of $(\mathbf{X'X})^{-1}$, reject null hypothesis if $|t_0| > t_{\alpha/2, n-p}$ 
- **Standardized Residuals:** $d_i = \frac{e_i}{\sqrt{MS_E}} = \frac{e_i}{\hat{\sigma}}$ 

**CH 15.**
- **Quality:** *fitness for use:* determined through interaction of *quality of design* (different grades/levels of performance, reliability, serviceability, and function that are the result of deliberate engineering and management decisions) and *quality of conformance* (systemic reduction of variability and elimination of defects). 
- **Basic Principles:** in any production process, regardless of how well designed or carefully maintained it is, a certain amount of inherent or natural variability always exists. In the framework of statistical quality control, this is often called a "stable system of chance causes." A process that is operating with **only chance causes** of variation present is said to be in *statistical control*. 
- **Assignable Causes:** sources of variability that are not part of the chance cause pattern; a process that is operating in the presence of assignable causes is said to be *out of control*.
- **Control Chart:** *center line* - represents the average value of the quality characteristic corresponding to the in-control state. *upper control limit and lower control limit* - these horizontal lines are chose so that if the process is in control, nearly all of the sample points fall between them.
	- A point that plots outside of the 3-sigma control limits is interpreted as evidence that the process is out of control.
	- two of three consecutive points plot beyond a 2-sigma limit
	- 4 of 5 consecutive points plot at a distance of 1 sigma or beyond from the center line
	- a run of length 8 or more points should be taken as a signal of an out-of-control condition (run = sequence of observations of the same type; could be increasing, decreasing, same side of the limit, etc.)
	- **General Model:** $UCL = \mu_W + k \sigma_W; CL = \mu_W; LCL = \mu_W - k \sigma_W$, a common choice is k = 3. $W$ is a sample statistic that measures some quality characteristic of interest.
- When dealing with a quality characteristic that can be expressed as a measurement, monitoring both the mean value of the quality characteristic and its variability is customary. mean value - $\overline{X}$ chart, variability - $R$ chart or $S$ chart.
- Suppose that $m$ preliminary samples are available, each of size $n$. Let the sample mean for the $i$th sample be $\overline{X}_i$. We estimate the mean of the population, $\mu$, by the **grand mean:** $\hat{\mu} = \overline{\overline{X}} = \frac{1}{m} \sum^m_{i=1} \overline{X}_i$. Thus, we may take $\overline{\overline{X}}$ as the center line on the $\overline{X}$ chart.
- Let $R_i$ be the range of the $i$th sample, and let $\overline{R} = \frac{1}{m} \sum^m_{i=1} R_i$ be the average range. $\hat{\sigma} = \frac{\overline{R}}{d_2}$. $A_2 = \frac{3}{d_2\sqrt{n}}$.
- **$\overline{X}$ Chart from R Chart**: $UCL = \overline{\overline{x}} + A_2 \overline{r} \ \  CL = \overline{\overline{x}} \ \ LCL = \overline{\overline{x}} - A_2 \overline{r}$
- **R Chart:** $UCL = D_4 \overline{r} \ \ C: = \overline{r} \ \ LCL = D_3 \overline{r}$ 
- Rather than base control charts on ranges, a more modern approach is to calculate and plot the stdev of each subgroup. Let $S_i$ denote the stdev of the $i$th sample. Define $\overline{S} = \frac{1}{m} \sum^m_{i=1} S_i$, $\hat{\sigma} = \overline{S}/c_4$, where $c_4$ depends on $n$. 
- **$\overline{X}$ Chart from S Chart**: $UCL = \overline{\overline{x}} + 3 \frac{\overline{s}}{c_4\sqrt{n}} \ \  CL = \overline{\overline{x}} \ \ LCL = \overline{\overline{x}} - 3 \frac{\overline{s}}{c_4\sqrt{n}}$
- **S Chart:** $UCL = \overline{s} + 3 \frac{\overline{s}}{c_4}\sqrt{1-c_4^2} \ \ CL = \overline{s} \ \ LCL = \overline{s} - 3 \frac{\overline{s}}{c_4}\sqrt{1-c_4^2}$